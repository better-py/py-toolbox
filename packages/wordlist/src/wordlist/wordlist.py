import json
from collections import OrderedDict
from loguru import logger
from src.wordlist.utils import path_jump_to, path_search, traverse_dir


class WordListTool(object):
    """
    è¯åº“å·¥å…·, æ¸…æ´—ä¸€äº›è¯åº“æ•°æ®ï¼ˆCET4,CET6, GRE, TOEFL, IELTS, etc.ï¼‰
    """

    def __init__(self, root_dir: str = None, resource_dir: str = None):
        self._root_dir = root_dir or "toolbox"
        self._resource_dir = resource_dir or "tmp"
        self._dist_dir = "dist"

        # é»˜è®¤çš„è¯åº“æºæ ¹è·¯å¾„æ˜¯ï¼š tmp/
        self.resource_path = path_search(self._root_dir).joinpath(self._resource_dir)
        # é»˜è®¤å¤„ç†ç»“æœæ ¹è·¯å¾„ï¼štmp/dist/
        self.dist_path = self.resource_path.joinpath(self._dist_dir)

        # mkdir:
        self.dist_path.mkdir(parents=True, exist_ok=True)

        logger.debug(f"resource_path: {self.resource_path}, dist_path: {self.dist_path}")

        #
        # vocabularies: åˆ†çº§è¯åº“
        #
        self.words = {
            "cet4": set(),  # è‹±è¯­4çº§
            "cet6": set(),  # è‹±è¯­6çº§
            "toefl": set(),  # æ‰˜ç¦
            "ielts": set(),  # é›…æ€
            "gre": set(),  # GRE
            #
            # merge:
            #
            "cet4_cet6": set(),  # è‹±è¯­4çº§ + è‹±è¯­6çº§
            "cet4_cet6_toefl": set(),  # è‹±è¯­4çº§ + è‹±è¯­6çº§ + æ‰˜ç¦
            "cet4_cet6_toefl_ielts": set(),  # è‹±è¯­4çº§ + è‹±è¯­6çº§ + æ‰˜ç¦ + é›…æ€
            "cet4_cet6_toefl_ielts_gre": set(),  # è‹±è¯­4çº§ + è‹±è¯­6çº§ + æ‰˜ç¦ + é›…æ€ + GRE
        }

    def parse_xlsx(self, filename: str):
        """è§£æ xlsx æ–‡ä»¶
        :param filename: æ–‡ä»¶å
        :return:
        """
        pass

    def parse_txt(self):

        res = "IELTS/IELTS Word List.txt"
        file = self.resource_path.joinpath(res)

        # parse
        words = {}
        with open(file, "r") as fp:
            for line in fp.readlines():
                line = line.strip()
                if not line:
                    continue

                # clean line:
                # start with alphabet:
                if not line[0].isascii():
                    logger.warning(f"âš ï¸ skip line: {line}")
                    continue
                if line.find("Word List") != -1:
                    logger.warning(f"âš ï¸ skip line: {line}")
                    continue

                # =============

                # if line.count("/") == 2:
                #     # ok:
                #     ret = line.split("/", maxsplit=2)
                #     if len(ret) != 3:
                #         logger.warning(f"ğŸ„ï¸ skip line: {line}")
                #         continue
                #     word, pronounce, meaning = ret
                #     word = word.strip("*")

                # ok:
                ret = line.split(maxsplit=2)
                if len(ret) != 3:
                    logger.warning(f"âš ï¸ skip line: {len(ret)}, {ret}")
                    continue
                word, pronounce, meaning = ret
                word = word.lower().strip("*")  # å°å†™+å»æ‰*

                #
                # fix word:
                #
                if pronounce.count("/") == 0 and pronounce.count("[") == 0 and pronounce.count("{") == 0:
                    word = f"{word} {pronounce}".strip("*")  # çŸ­è¯­
                    pronounce = ""
                    if meaning.count("*") > 0:
                        head, tail = meaning.split("*", maxsplit=1)
                        word = f"{word} {head}".strip("*")  # çŸ­è¯­
                        meaning = tail

                    logger.warning(f"âœ…ï¸ fix word: {word}, {pronounce}, {meaning}")
                elif pronounce.count("/") == 1 and meaning.count("/") > 0:  # éŸ³æ ‡åˆ‡åˆ†å¼‚å¸¸ï¼Œä»è¯ä¹‰ä¸­æå–éŸ³æ ‡éƒ¨åˆ†
                    p_fix, meaning = meaning.split("/", maxsplit=1)
                    pronounce = f"{pronounce}{p_fix}/"
                    logger.warning(f"â›”ï¸ fix pronounce: {word}, {pronounce}, {meaning}")

                # ç»Ÿä¸€éŸ³æ ‡æ ¼å¼:
                if pronounce.count("{") > 0:
                    pronounce = pronounce.replace("{", "/").replace("}", "/")
                elif pronounce.count("[") > 0:
                    pronounce = pronounce.replace("[", "/").replace("]", "/")

                words[word] = {
                    "pronounce": pronounce.strip(),
                    "meaning": meaning.strip(),
                }

        # ====================
        logger.debug(f"words: {len(words)}")
        return words

    def save_csv(self, data: dict):
        """ä¿å­˜ csv æ–‡ä»¶
        :return:
        """

        dist_file = self.dist_path.joinpath("ielts.csv")  # é›…æ€è¯æ±‡è¡¨ + å‘éŸ³ + è¯ä¹‰

        # sort:
        data = OrderedDict(sorted(data.items(), key=lambda x: x[0]))

        # save to csv:
        with open(dist_file, "w") as fp:
            for k, v in data.items():
                fp.write(f"{k}, {v['pronounce']}, {v['meaning']}\n")

    def parse_vocabulary_by_dirs(self):
        """è§£æè¯åº“æ–‡ä»¶

        :param
        :return:
        """

        self.parse_dirs()
        self.save_files()

    def handle2(self):
        """å¤„ç†è¯åº“æ–‡ä»¶
        :return:
        """
        # parse:
        self.parse_dir_files()
        self.save_files("2")

    def handle3(self):
        words = self.parse_txt()
        self.save_csv(words)

    def save_files(self, file_suffix: str = None):
        """ä¿å­˜æ–‡ä»¶

        :param file_suffix: æ–‡ä»¶åç¼€
        :return:
        """
        for k, v in self.words.items():
            kk = f"{k}" if not file_suffix else f"{k}_{file_suffix}"
            f_txt = self.dist_path.joinpath(f"{kk}.txt")
            f_json = self.dist_path.joinpath(f"{kk}.json")

            # sort:
            v = sorted(v)

            # save to json:
            with open(f_json, "w") as fp:
                fp.write(json.dumps(v))

            # save to txt:
            with open(f_txt, "w") as fp:
                for word in v:
                    fp.write(word + "\n")

    def parse_dir_files(self):
        res_dirs = {
            "cet4": "English-words-cards/CET4/manifest.json",
            "cet6": "English-words-cards/CET6/manifest.json",
            "toefl": "English-words-cards/TOEFL/manifest.json",
            "ielts": "English-words-cards/IELTS/manifest.json",
            "gre": "English-words-cards/GRE/manifest.json",
        }

        # parse json file:
        for k, v in res_dirs.items():
            res_dir = self.resource_path.joinpath(v)

            words = set()
            with open(res_dir, "r") as fp:
                data = json.loads(fp.read())
                items = data["items"] or []

                for item in items:
                    v = item["url"] or ""
                    if not v:
                        continue

                    # fmt:
                    word = v.lower().rsplit("/", 1)[-1].removesuffix(".png")
                    # logger.debug(f"word: {word}")
                    if word in words:
                        logger.warning(f"word: {word} already exist in {res_dir}.")
                    # save set:
                    words.add(word)

            # save:
            logger.debug(f"words num: {len(words)}, dir: {res_dir}")
            self.words[k] = sorted(words)

        # ===================================================

        self.merge_words()

    def parse_dirs(self, dirs: str = None, file_type: str = ".png"):
        """è§£æç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶

        :param dirs: ç›®å½•è·¯å¾„
        :param file_type: ç›®æ ‡æ–‡ä»¶ç±»å‹
        """

        res_dirs = dirs or {
            "cet4": "English-words-cards/CET4/images",
            "cet6": "English-words-cards/CET6/images",
            "toefl": "English-words-cards/TOEFL/images",
            "ielts": "English-words-cards/IELTS/images",
            "gre": "English-words-cards/GRE/images",
        }

        for k, v in res_dirs.items():
            res_dir = self.resource_path.joinpath(v)

            words = set()
            for f in res_dir.iterdir():
                if f.is_file() and f.suffix == file_type:
                    word = f.name.lower().removesuffix(file_type)
                    # if exist
                    if word in words:
                        logger.warning(f"word: {word} already exist in {res_dir}.")
                    words.add(word)

            logger.debug(f"words: {len(words)}, dir: {res_dir}")
            # sorted
            self.words[k] = sorted(words)

        # ===================================================

        # merge words:
        self.merge_words()

    def merge_words(self):
        # merge words:
        merge_words = set()
        # add cet4
        merge_words.update(self.words["cet4"])
        merge_words.update(self.words["cet6"])

        self.words["cet4_cet6"] = set(sorted(merge_words))
        logger.debug(f"words union: cet4_cet6 = {len(self.words['cet4_cet6'])}")

        # add toefl
        merge_words.update(self.words["toefl"])
        self.words["cet4_cet6_toefl"] = set(sorted(merge_words))
        logger.debug(f"words union: cet4_cet6_toefl = {len(self.words['cet4_cet6_toefl'])}")

        # add ielts
        merge_words.update(self.words["ielts"])
        self.words["cet4_cet6_toefl_ielts"] = set(sorted(merge_words))
        logger.debug(f"words union: cet4_cet6_toefl_ielts = {len(self.words['cet4_cet6_toefl_ielts'])}")

        # add gre
        merge_words.update(self.words["gre"])
        self.words["cet4_cet6_toefl_ielts_gre"] = set(sorted(merge_words))
        logger.debug(f"words union: cet4_cet6_toefl_ielts_gre = {len(self.words['cet4_cet6_toefl_ielts_gre'])}")
